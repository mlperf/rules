:toc:
:toclevels: 4

:sectnums:

= MLPerf HPC Training Rules

Version 0.7 
July 16, 2020

Points of contact: David Kanter (dkanter@gmail.com), Steve Farrell (sfarrell@lbl.gov)

== Overview

All rules are taken from the MLPerf Training Rules except for those that are overridden here.

The MLPerf name and logo are trademarks. In order to refer to a result using the
MLPerf name, the result must conform to the letter and spirit of the rules
specified in this document. The MLPerf organization reserves the right to solely
determine if a use of its name or logo is acceptable.

== Benchmarks

The benchmark suite consists of the benchmarks shown in the following table.

|===
|Problem |Dataset |Quality Target
|Climate segmentation |CAM5+TECA climate simulation with 3 target classes (atmospheric river, tropical cyclone, background) |IOU 0.82
|Cosmological parameter prediction |CosmoFlow N-body cosmological simulation data with 4 cosmological parameter targets |Mean average error 0.124
|===

== Divisions

There are two divisions of the HPC benchmark suite, the Closed division and the Open division.

=== Closed Division

The Closed division requires using the same preprocessing, model, and training method as the reference implementation.

The closed division models are:

|===
|Problem |Model
|Climate segmentation  |https://github.com/azrael417/mlperf-deepcam
|Cosmological parameter prediction |https://github.com/sparticlesteve/cosmoflow-benchmark
|===

== Data Set

=== Data State at Start of Run

Each reference implementation includes a download script or broadly available method to acquire and verify the dataset.

The data at the start of the benchmark run should reside on a parallel file system that is persistent (>= 1 month, not subject to eviction by other users), can be downloaded to / accessed by the user, and can be shared among users at the facility. Any staging to node-local disk or memory or system burst buffer should be included in the benchmark time measurement.

You must flush/reset the on-node caches prior to benchmarking. Due to practicality issues, you are not required to reset off-node system-level caches.

We otherwise follow the training rule on consistency with the reference implementation preprocessing and allowance for reformatting.

== Training Loop

=== Hyperparameters and Optimizer

CLOSED:

Allowed hyperparameter and optimizer settings will be specified in the MLPerf HPC v0.5 Hyperparameter List. https://docs.google.com/spreadsheets/d/1jxsas4RRxKoKmIYcFZcFCQ1ZLnaMLSU8B5g5ZDKQ7sE/edit?usp=sharing

|===
 |Model |Name |Constraint |Definition |Reference Code
 |CosmoFlow |global_batch_size |unconstrained |the global batch size for training |local `batch_size` (`--batch-size`) times number of workers. Baseline config is 64
 |CosmoFlow |opt_name |"sgd" |the optimizer name |`--optimizer` or link:https://github.com/sparticlesteve/cosmoflow-benchmark/blob/57c2454a28e415ca7df0135f016297763f6e4946/configs/cosmo.yaml#L33[config]
 |CosmoFlow |sgd_opt_momentum |0.9 |SGD momentum |link:https://github.com/sparticlesteve/cosmoflow-benchmark/blob/57c2454a28e415ca7df0135f016297763f6e4946/configs/cosmo.yaml#L34[config]
 |CosmoFlow |opt_base_learning_rate |unconstrained |The base learning rate |`base_lr` times scaling factor, e.g. `global_batch_size/base_batch_size` if scaling="linear". link:https://github.com/sparticlesteve/cosmoflow-benchmark/blob/57c2454a28e415ca7df0135f016297763f6e4946/configs/cosmo.yaml#L38[Config]
 |CosmoFlow |opt_learning_rate_warmup_epochs |unconstrained |the number of epochs for learning rate to warm up to base value |link:https://github.com/sparticlesteve/cosmoflow-benchmark/blob/57c2454a28e415ca7df0135f016297763f6e4946/configs/cosmo.yaml#L47[config]
 |CosmoFlow |opt_learning_rate_warmup_factor |unconstrained |the constant factor applied at learning rate warm up |scaled learning rate / `base_lr`
 |CosmoFlow |opt_learning_rate_decay_boundary_epochs |[i, j], where i, j are positive integers, i.e. two learning rate decays are allowed in training" |Epochs at which learning rate decays |link:https://github.com/sparticlesteve/cosmoflow-benchmark/blob/57c2454a28e415ca7df0135f016297763f6e4946/configs/cosmo.yaml#L51[config]
 |CosmoFlow |opt_learning_rate_decay_factor |`0 < value < 1`, and you may use a different value for each decay |the learning rate decay factor(s) at the decay boundary epochs |link:https://github.com/sparticlesteve/cosmoflow-benchmark/blob/57c2454a28e415ca7df0135f016297763f6e4946/configs/cosmo.yaml#L51[config]
 |DeepCAM |global_batch_size |unconstrained |the global batch size for training |`--local_batch_size` times number of workers
 |DeepCAM |opt_name |AdamW or LAMB |the optimizer name |`--optimizer`
 |DeepCAM |opt_epsilon |1e-6 or 1e-8 |epsilon for Adam |`--adam_eps`
 |DeepCAM |opt_base_learning_rate |unconstrained |the base learning rate |`--start_lr` times warmup factor
 |DeepCAM |opt_learning_rate_warmup_steps |unconstrained |the number of epochs for learning rate to warm up to base value |`--lr_warmup_steps`
 |DeepCAM |opt_learning_rate_warmup_factor |unconstrained |the constant factor applied at learning rate warmup |`--lr_warmup_factor`
 |DeepCAM |opt_learning_rate_decay_steps |unconstrained |the steps at which learning rate is decayed |milestones in `--lr_schedule type="multistep",milestones="3000 10000",decay_rate="0.1"`
 |DeepCAM |opt_learning_rate_decay_factor |unconstrained |the learning rate decay factor |decay_rate in `--lr_schedule type="multistep",milestones="15000 25000",decay_rate="0.1"`
 |DeepCAM |opt_weight_decay |0.01 |L2 weight decay |`--weight_decay`
 |DeepCAM |validation_frequency |100 |number of steps between model validation |`--validation_frequency`
 |DeepCAM |loss_weight_pow |-0.125 |negative loss weight |`--loss_weight_pow`
|===

OPEN: Hyperparameters and optimizer may be freely changed.

== Run Results

MLPerf Training submissions consists of the following three metrics. Metrics 1 and 2 are considered mandatory for a complete submission whereas metric 3 is considered optional:


=== Strong Scaling (Time to Convergence)
This is a *mandatory* metric: see MLPerf Training xref:training_rules.adoc#section-run-results[Rule 11] for reference. The same rules apply here.

=== Data staging
This is a *mandatory* metric: staging of data from remote distributed storage to node-local storage or I/O accelerators must be timed as described in <<Data State at Start of Run>> above.

=== Weak Scaling (Throughput)
This is an *optional* metric. It was designed to test the training capacity of a system.

Measurement: the submitter picks a number of independent training instances K and machine scale S. This scale can be defined as:
* number of nodes
* number of accelerators (e.g. GPU)
* number of canonical compute units for a given system

We will use the term compute unit as a generic term. Note that S can be different from those used for submitting metric 1. 
Ideally S is the scale of the full system but this is not required.  

Each training instance will cover S/K compute units. The submitter trains these K instances to convergence. 

We define a Time-To-Train-all (TTTa) number by computing the difference between the end time of the instance which needs longest time to converge and the start time of the instance which starts up fastest. Mathematically this can be expressed as 

----
TTTa = max(run_stop) - min(run_start) where the max/min are taken over all instances K. 
----

Reporting: the submitter reports the the tuple (K, TTTa). 

Restrictions: 

* The submitter *must not report this score on it's own*. It has to be reported in conjunction with the scores from <<Strong Scaling (Time to Convergence)>> and <<Data staging>>.
* this score *does not allow for extrapolation*. All K training instances have to be run to convergence at chosen scale S.



== Benchmark Results

We follow the MLPerf Training Rule 11 along with the following required number of runs per benchmark.
Note that since run-to-run variability is already captured by spatial multiplexing in case of metric 3, the number of required runs can be reduced. More precisely, the submitter has to obey the following rule:

|===
|Benchmark |Number of Runs (Metric 1, 2) | Instance Count x Number of Runs (Metric 3)
|DeepCAM | 5 | >=5
|CosmoFlow | 10 | >=10
|===
